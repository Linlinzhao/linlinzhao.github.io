<!DOCTYPE html><html lang="en">
  <head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><title>Understanding backward() in PyTorch (Updated for V0.4) - Linlin's blog</title>

<meta name="description" content="Update for PyTorch 0.4:Earlier versions used Variable to wrap tensors with different properties. Since version 0.4, Variable is merged with tensor, in other ...">
<link rel="canonical" href="/tech/2017/10/24/understanding-backward()-in-PyTorch.html"><link rel="alternate" type="application/rss+xml" title="Linlin's blog" href="/feed.xml"><!-- start favicons snippet, use https://realfavicongenerator.net/ -->


<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/assets/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/browserconfig.xml">
<meta name="theme-color" content="#ffffff">



<!--<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#fc4d50"><link rel="shortcut icon" href="/assets/favicon.ico">

<meta name="msapplication-TileColor" content="#ffc40d"><meta name="msapplication-config" content="/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff">-->
<!-- end favicons snippet --><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.1/css/all.css" ><!-- start custom head snippets -->

<!-- end custom head snippets -->
<script>(function() {
  window.isArray = function(val) {
    return Object.prototype.toString.call(val) === '[object Array]';
  };
  window.isString = function(val) {
    return typeof val === 'string';
  };

  window.hasEvent = function(event) {
    return 'on'.concat(event) in window.document;
  };

  window.isOverallScroller = function(node) {
    return node === document.documentElement || node === document.body || node === window;
  };

  window.isFormElement = function(node) {
    var tagName = node.tagName;
    return tagName === 'INPUT' || tagName === 'SELECT' || tagName === 'TEXTAREA';
  };

  window.pageLoad = (function () {
    var loaded = false, cbs = [];
    window.addEventListener('load', function () {
      var i;
      loaded = true;
      if (cbs.length > 0) {
        for (i = 0; i < cbs.length; i++) {
          cbs[i]();
        }
      }
    });
    return {
      then: function(cb) {
        cb && (loaded ? cb() : (cbs.push(cb)));
      }
    };
  })();
})();
(function() {
  window.throttle = function(func, wait) {
    var args, result, thisArg, timeoutId, lastCalled = 0;

    function trailingCall() {
      lastCalled = new Date;
      timeoutId = null;
      result = func.apply(thisArg, args);
    }
    return function() {
      var now = new Date,
        remaining = wait - (now - lastCalled);

      args = arguments;
      thisArg = this;

      if (remaining <= 0) {
        clearTimeout(timeoutId);
        timeoutId = null;
        lastCalled = now;
        result = func.apply(thisArg, args);
      } else if (!timeoutId) {
        timeoutId = setTimeout(trailingCall, remaining);
      }
      return result;
    };
  };
})();
(function() {
  var Set = (function() {
    var add = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (data[i] === item) {
          return;
        }
      }
      this.size ++;
      data.push(item);
      return data;
    };

    var Set = function(data) {
      this.size = 0;
      this._data = [];
      var i;
      if (data.length > 0) {
        for (i = 0; i < data.length; i++) {
          add.call(this, data[i]);
        }
      }
    };
    Set.prototype.add = add;
    Set.prototype.get = function(index) { return this._data[index]; };
    Set.prototype.has = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (this.get(i) === item) {
          return true;
        }
      }
      return false;
    };
    Set.prototype.is = function(map) {
      if (map._data.length !== this._data.length) { return false; }
      var i, j, flag, tData = this._data, mData = map._data;
      for (i = 0; i < tData.length; i++) {
        for (flag = false, j = 0; j < mData.length; j++) {
          if (tData[i] === mData[j]) {
            flag = true;
            break;
          }
        }
        if (!flag) { return false; }
      }
      return true;
    };
    Set.prototype.values = function() {
      return this._data;
    };
    return Set;
  })();

  window.Lazyload = (function(doc) {
    var queue = {js: [], css: []}, sources = {js: {}, css: {}}, context = this;
    var createNode = function(name, attrs) {
      var node = doc.createElement(name), attr;
      for (attr in attrs) {
        if (attrs.hasOwnProperty(attr)) {
          node.setAttribute(attr, attrs[attr]);
        }
      }
      return node;
    };
    var end = function(type, url) {
      var s, q, qi, cbs, i, j, cur, val, flag;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        s[url] = true;
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (cur.urls.has(url)) {
            qi = cur, val = qi.urls.values();
            qi && (cbs = qi.callbacks);
            for (flag = true, j = 0; j < val.length; j++) {
              cur = val[j];
              if (!s[cur]) {
                flag = false;
              }
            }
            if (flag && cbs && cbs.length > 0) {
              for (j = 0; j < cbs.length; j++) {
                cbs[j].call(context);
              }
              qi.load = true;
            }
          }
        }
      }
    };
    var load = function(type, urls, callback) {
      var s, q, qi, node, i, cur,
        _urls = typeof urls === 'string' ? new Set([urls]) : new Set(urls), val, url;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (_urls.is(cur.urls)) {
            qi = cur;
            break;
          }
        }
        val = _urls.values();
        if (qi) {
          callback && (qi.load || qi.callbacks.push(callback));
          callback && (qi.load && callback());
        } else {
          q.push({
            urls: _urls,
            callbacks: callback ? [callback] : [],
            load: false
          });
          for (i = 0; i < val.length; i++) {
            node = null, url = val[i];
            if (s[url] === undefined) {
              (type === 'js' ) && (node = createNode('script', { src: url }));
              (type === 'css') && (node = createNode('link', { rel: 'stylesheet', href: url }));
              if (node) {
                node.onload = (function(type, url) {
                  return function() {
                    end(type, url);
                  };
                })(type, url);
                (doc.head || doc.body).appendChild(node);
                s[url] = false;
              }
            }
          }
        }
      }
    };
    return {
      js: function(url, callback) {
        load('js', url, callback);
      },
      css: function(url, callback) {
        load('css', url, callback);
      }
    };
  })(this.document);
})();
</script><script>
  (function() {
    var TEXT_VARIABLES = {
      version: '2.2.6',
      sources: {
        font_awesome: 'https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.1/css/all.css',
        jquery: 'https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js',
        leancloud_js_sdk: '//cdn.jsdelivr.net/npm/leancloud-storage@3.13.2/dist/av-min.js',
        chart: 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js',
        gitalk: {
          js: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.js',
          css: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.css'
        },
        valine: 'https://unpkg.com/valine/dist/Valine.min.js',
        mathjax: 'https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML',
        mermaid: 'https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js'
      },
      site: {
        toc: {
          selectors: 'h1,h2,h3'
        }
      },
      paths: {
        search_js: '/assets/search.js'
      }
    };
    window.TEXT_VARIABLES = TEXT_VARIABLES;
  })();
</script>
</head>
  <body>
    <div class="root" data-is-touch="false">
      <div class="layout--page js-page-root"><div class="page__main js-page-main page__viewport has-aside cell cell--auto">

      <div class="page__main-inner"><div class="page__header d-print-none"><header class="header"><div class="main">
      <div class="header__title">
        <div class="header__brand"><?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="50pt" height="50pt" viewBox="0 0 50 50" version="1.1">
<g id="surface1">
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;" d="M 49.8125 24.96875 C 49.8125 38.597656 38.765625 49.648438 25.136719 49.648438 C 11.503906 49.648438 0.457031 38.597656 0.457031 24.96875 C 0.457031 11.339844 11.503906 0.289062 25.136719 0.289062 C 38.765625 0.289062 49.8125 11.339844 49.8125 24.96875 Z M 49.8125 24.96875 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(0%,0%,0%);fill-opacity:1;" d="M 23.300781 49.742188 C 12.609375 49.1875 2.980469 40.855469 0.789062 30.402344 C -1.230469 21.671875 1.929688 12.011719 8.773438 6.214844 C 13.628906 1.992188 20.15625 -0.234375 26.582031 0.25 C 36.761719 0.664062 46.082031 8.207031 48.824219 17.992188 C 49.949219 21.589844 49.945312 25.449219 49.492188 29.152344 C 47.945312 38.582031 40.527344 46.730469 31.226562 49.007812 C 28.65625 49.640625 25.945312 49.96875 23.300781 49.742188 Z M 19.492188 48.222656 C 14.957031 45.996094 12.152344 40.835938 12.796875 35.820312 C 13.308594 29.902344 18.691406 25.015625 24.605469 24.910156 C 29.558594 25.0625 34.441406 21.984375 36.242188 17.335938 C 38.398438 12.253906 36.535156 5.875 31.957031 2.773438 C 28.207031 0.0078125 23.183594 0.59375 18.914062 1.554688 C 9.042969 3.972656 1.410156 13.21875 0.859375 23.359375 C 0.140625 32.511719 5.179688 41.75 13.242188 46.128906 C 15.261719 47.21875 18.128906 48.496094 20.015625 48.519531 L 19.808594 48.394531 Z M 24.320312 15.75 C 21.820312 15.269531 21.066406 11.597656 23.21875 10.207031 C 25.429688 8.488281 28.898438 10.828125 28.027344 13.554688 C 27.628906 15.125 25.890625 16.113281 24.320312 15.75 Z M 25.574219 40.082031 C 28.640625 39.699219 28.71875 34.789062 25.660156 34.324219 C 23.238281 33.691406 21.132812 36.71875 22.5625 38.78125 C 23.167969 39.699219 24.460938 40.425781 25.574219 40.082031 Z M 25.574219 40.082031 "/>
</g>
</svg>
<a title="" href="/">Linlin's blog</a></div><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></div><nav class="navigation">
        <ul><li class="navigation__item"><a href="/archive.html">Archive</a></li><li class="navigation__item"><a href="/about.html">About</a></li><li><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></li></ul>
      </nav></div>
  </header>
</div><div class="page__content"><div class ="main"><div class="grid grid--reverse">

              <div class="col-aside d-print-none js-col-aside"><aside class="page__aside js-page-aside"><div class="toc-aside js-toc-root"></div>
</aside></div>

              <div class="col-main cell cell--auto"><!-- start custom main top snippet -->

<!-- end custom main top snippet -->
<article itemscope itemtype="http://schema.org/Article"><div class="article__header"><header><h1>Understanding backward() in PyTorch (Updated for V0.4)</h1></header><span class="split-space">&nbsp;</span>
          <a class="edit-on-github"
            title="Edit on Github"
            href="https://github.com/Linlinzhao/linlinzhao.github.io/tree/master/_posts/2017-10-21-understanding-backward()-in-PyTorch.md">
            <i class="far fa-edit"></i></a></div><meta itemprop="headline" content="Understanding backward() in PyTorch (Updated for V0.4)"><div class="article__info clearfix"><ul class="left-col menu"><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=PyTorch">PyTorch</a>
            </li><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=machine-learning">machine-learning</a>
            </li></ul><ul class="right-col menu"><li><i class="far fa-calendar-alt"></i> <span>Oct 24, 2017</span>
            </li></ul></div><meta itemprop="author" content="Linlin Zhao"/><meta itemprop="datePublished" content="2017-10-24T00:00:00+02:00">
    <meta itemprop="keywords" content="PyTorch,machine-learning"><div class="js-article-content"><div class="layout--article"><!-- start custom article top snippet -->

<!-- end custom article top snippet -->
<div class="article__content" itemprop="articleBody"><p>Update for PyTorch 0.4:</p>

<p>Earlier versions used <code class="language-plaintext highlighter-rouge">Variable</code> to wrap tensors with different properties. Since version 0.4, <code class="language-plaintext highlighter-rouge">Variable</code> is merged with <code class="language-plaintext highlighter-rouge">tensor</code>, in other words, <code class="language-plaintext highlighter-rouge">Variable</code> is NOT needed anymore. The flag <code class="language-plaintext highlighter-rouge">require_grad</code> can be directly set in <code class="language-plaintext highlighter-rouge">tensor</code>. Accordingly, this post is also updated.</p>

<hr />

<!--more-->
<p>Having heard about the announcement about Theano from Bengio lab , as a Theano user, I am happy and sad to see the fading of the old hero, caused by many raising stars. Sad to see it is too old to compete with its industrial competitors, and happy to have so many excellent deep learning frameworks to choose from. Recently I started translating some of my old codes to Pytorch and have been really impressed by its dynamic nature and clearness. But at the very beginning, I was very confused by the <code class="language-plaintext highlighter-rouge">backward()</code> function when reading the tutorials and documentations. This motivated me to write this post in order for other Pytorch beginners to ease the understanding a bit. And I’ll assume that you already know the <a href="http://pytorch.org/docs/master/autograd.html"><code class="language-plaintext highlighter-rouge">autograd</code></a> module and what a <a href="http://pytorch.org/docs/0.1.12/_modules/torch/autograd/variable.html"><code class="language-plaintext highlighter-rouge">Variable</code></a> is, but are a little confused by definition of <code class="language-plaintext highlighter-rouge">backward()</code>.</p>

<p>First let’s recall the gradient computing under mathematical notions. 
For an independent variable \(x\) (scalar or vector), the whatever operation on \(x\) is \(y = f(x)\). Then the gradient of \(y\) w.r.t \(x_i\)s is</p>

\[\begin{align}
\nabla y&amp;=\begin{bmatrix}
\frac{\partial y}{\partial x_1}\\
\frac{\partial y}{\partial x_2}\\
\vdots
\end{bmatrix}
\end{align}.\]

<p>Then for a specific point of \(x=[X_1, X_2, \cdots]\), we’ll get the gradient of \(y\) on that point as a vector. 
With these notions in mind, the following things are a bit confusing at the beginning</p>

<ol>
  <li>
    <p>Mathematically, we would say “The gradients of a function w.r.t. the independent variables”, whereas the <code class="language-plaintext highlighter-rouge">.grad</code> is attached to the leaf <code class="language-plaintext highlighter-rouge">tensor</code>s. In Theano and Tensorflow, the computed gradients are stored separately in a variable. But with a moment of adjustment, it is fairly easy to buy that. In Pytorch it is also possible to get the <code class="language-plaintext highlighter-rouge">.grad</code> for intermediate <code class="language-plaintext highlighter-rouge">Variable</code>s with help of <code class="language-plaintext highlighter-rouge">register_hook</code> function</p>
  </li>
  <li>
    <p>The parameter <code class="language-plaintext highlighter-rouge">grad_variables</code> of the function <code class="language-plaintext highlighter-rouge">torch.autograd.backward(variables, grad_tensors=None, retain_graph=None, create_graph=None, retain_variables=None, grad_variables=None)</code> is not straightforward for knowing its functionality. **note that <code class="language-plaintext highlighter-rouge">grad_variables</code> is deprecated, use <code class="language-plaintext highlighter-rouge">grad_tensors</code> instead.</p>
  </li>
  <li>
    <p>What is <code class="language-plaintext highlighter-rouge">retain_graph</code> doing?</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="n">T</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<h3 id="simplicity-of-using-backward">Simplicity of using <code class="language-plaintext highlighter-rouge">backward()</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">'''
Define a scalar variable, set requires_grad to be true to add it to backward path for computing gradients

It is actually very simple to use backward()

first define the computation graph, then call backward()
'''</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#x is a leaf created by user, thus grad_fn is none
</span><span class="k">print</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#define an operation on x
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">#define one more operation to check the chain rule
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z'</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x tensor([[-0.6955]], requires_grad=True)
y tensor([[-1.3911]], grad_fn=&lt;MulBackward&gt;)
z tensor([[-2.6918]], grad_fn=&lt;PowBackward0&gt;)
</code></pre></div></div>

<p>The simple operations defined a forward path \(z=(2x)^3\), \(z\) will be the final output <code class="language-plaintext highlighter-rouge">tensor</code> we would like to compute gradient: \(dz=24x^2dx\), which will be passed to the parameter <code class="language-plaintext highlighter-rouge">tensors</code> in <code class="language-plaintext highlighter-rouge">backward()</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#yes, it is just as simple as this to compute gradients:
</span><span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'z gradient:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y gradient:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient:'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">,</span> <span class="s">'Requires gradient?'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span> <span class="c1"># note that x.grad is also a tensor
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z gradient None
y gradient None
x gradient tensor([[11.6105]]) Requires gradient? False
</code></pre></div></div>

<p>The gradients of both \(y\) and \(z\) are None, since the function returns the gradient for the leaves, which is \(x\) in this case. At the very beginning, I was assuming something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="n">gradient</span><span class="p">:</span> <span class="bp">None</span>

<span class="n">y</span> <span class="n">gradient</span><span class="p">:</span> <span class="bp">None</span>

<span class="n">z</span> <span class="n">gradient</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">11.6105</span><span class="p">])</span>
</code></pre></div></div>

<p>since the gradient is calculated for the final output \(z\).</p>

<p>With a blink of thinking, we could figure out it would be practically chaos if \(x\) is a multi-dimensional vector. <code class="language-plaintext highlighter-rouge">x.grad</code> should be interpreted as the gradient of \(z\) at \(x\).</p>

<h3 id="how-do-we-use-grad_tensors">How do we use <code class="language-plaintext highlighter-rouge">grad_tensors</code>?</h3>

<p><code class="language-plaintext highlighter-rouge">grad_tensors</code> should be a list of torch tensors. In default case, the <code class="language-plaintext highlighter-rouge">backward()</code> is applied to scalar-valued function, the default value of <code class="language-plaintext highlighter-rouge">grad_tensors</code> is thus <code class="language-plaintext highlighter-rouge">torch.FloatTensor([0])</code>. But why is that? What if we put some other values to it?</p>

<p>Keep the same forward path, then do <code class="language-plaintext highlighter-rouge">backward</code> by only setting <code class="language-plaintext highlighter-rouge">retain_graph</code> as <code class="language-plaintext highlighter-rouge">True</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#x is a leaf created by user, thus grad_fn is none
</span><span class="k">print</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#define an operation on x
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#define one more operation to check the chain rule
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">3</span>

<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Keeping the default value of grad_tensors gives'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z gradient:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y gradient:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient:'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x tensor([[-0.7207]], requires_grad=True)
Keeping the default value of grad_tensors gives
z gradient: None
y gradient: None
x gradient: tensor([[12.4668]])
</code></pre></div></div>

<p>Testing the explicit default value, which should give the same result. For the same graph which is retained, DO NOT forget to zero the gradient before recalculate the gradients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Set grad_tensors to 1 gives'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z gradient:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y gradient:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient:'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Set grad_tensors to 0 gives
z gradient: None
y gradient: None
x gradient: tensor([[12.4668]])
</code></pre></div></div>

<p>Then what about other values, let’s try 0.1 and 0.5.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Set grad_tensors to 0.1 gives'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z gradient:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y gradient:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient:'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Set grad_tensors to 0.1 gives
z gradient: None
y gradient: None
x gradient: tensor([[1.2467]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Modifying the default value of grad_variables to 0.1 gives'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z gradient'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y gradient'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Modifying the default value of grad_variables to 0.5 gives

z gradient None

y gradient None

x gradient tensor([[6.2334]])
</code></pre></div></div>

<p>It looks like the elements of <code class="language-plaintext highlighter-rouge">grad_tensors</code> act as scaling factors. Now let’s set \(x\) to be a \(2\times 2\) matrix. Note that \(z\) will also be a matrix. (Always use the latest version, <code class="language-plaintext highlighter-rouge">backward</code> had been improved a lot from earlier version, becoming much easier to understand.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#x is a leaf created by user, thus grad_fn is none
</span><span class="k">print</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#define an operation on x
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#define one more operation to check the chain rule
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">3</span>

<span class="k">print</span><span class="p">(</span><span class="s">'z shape:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient for its all elements:</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="k">print</span><span class="p">()</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1">#the gradient for x will be accumulated, it needs to be cleared.
</span><span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient for the second column:</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="k">print</span><span class="p">()</span>
<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient for the first row:</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x tensor([[-2.5212,  1.2730],
        [ 0.0366, -0.0750]], requires_grad=True)
z shape: torch.Size([2, 2])
x gradient for its all elements:
 tensor([[152.5527,  38.8946],
        [  0.0322,   0.1349]])

x gradient for the second column:
 tensor([[ 0.0000, 38.8946],
        [ 0.0000,  0.1349]])

x gradient for the first row:
 tensor([[152.5527,  38.8946],
        [  0.0000,   0.0000]])
</code></pre></div></div>

<p>We can clearly see the gradients of \(z\) are computed w.r.t to each dimension of \(x\), because the operations are all element-wise.</p>

<p>Then what if we render the output one-dimensional (scalar) while \(x\) is two-dimensional. This is a real simplified scenario of neural networks.</p>

\[f(x)=\frac{1}{n}\sum_i^n(2x_i)^3\]

\[f'(x)=\frac{1}{n}\sum_i^n24x_i^2\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#x is a leaf created by user, thus grad_fn is none
</span><span class="k">print</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#define an operation on x
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#print('y', y)
#define one more operation to check the chain rule
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'out'</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient:</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x tensor([[ 1.8528,  0.2083],
        [-1.5296,  0.3136]], requires_grad=True)
out tensor(5.6434, grad_fn=&lt;MeanBackward1&gt;)
x gradient:
 tensor([[20.5970,  0.2604],
        [14.0375,  0.5903]])
</code></pre></div></div>

<p>We will get complaints if the <code class="language-plaintext highlighter-rouge">grad_tensors</code> is specified for the scalar function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">T</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x gradient'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-78-db7cccdf3863&gt; in &lt;module&gt;()
      1 x.grad.data.zero_()
----&gt; 2 out.backward(T.FloatTensor([[1, 1], [1, 1]]), retain_graph=True)
      3 print('x gradient', x.grad)


/usr/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
     91                 products. Defaults to ``False``.
     92         """
---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)
     94 
     95     def register_hook(self, hook):


/usr/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     88     Variable._execution_engine.run_backward(
     89         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 90         allow_unreachable=True)  # allow_unreachable flag
     91 
     92 


RuntimeError: invalid gradient at index 0 - expected shape [] but got [2, 2]
</code></pre></div></div>

<h3 id="what-is-retain_graph-doing">What is <code class="language-plaintext highlighter-rouge">retain_graph</code> doing?</h3>

<p>When training a model, the graph will be re-generated for each iteration. Therefore each iteration will consume the graph if the <code class="language-plaintext highlighter-rouge">retain_graph</code> is false, in order to keep the graph, we need to set it be true.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#x is a leaf created by user, thus grad_fn is none
</span><span class="k">print</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1">#define an operation on x
</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1">#print('y', y)
#define one more operation to check the chain rule
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'out'</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1">#without setting retain_graph to be true, it is alright for first time of backward.
</span><span class="k">print</span><span class="p">(</span><span class="s">'x gradient'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

<span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1">#Now we get complaint saying that no graph is available for tracing back. 
</span><span class="k">print</span><span class="p">(</span><span class="s">'x gradient'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x tensor([[-0.7452,  1.5727],
        [ 0.1702,  0.7374]], requires_grad=True)
out tensor(7.7630, grad_fn=&lt;MeanBackward1&gt;)
x gradient tensor([[ 3.3323, 14.8394],
        [ 0.1738,  3.2623]])



---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-82-80a8d867d529&gt; in &lt;module&gt;()
     12 
     13 x.grad.data.zero_()
---&gt; 14 out.backward() #Now we get complaint saying that no graph is available for tracing back.
     15 print('x gradient', x.grad)


/usr/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
     91                 products. Defaults to ``False``.
     92         """
---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)
     94 
     95     def register_hook(self, hook):


/usr/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     88     Variable._execution_engine.run_backward(
     89         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 90         allow_unreachable=True)  # allow_unreachable flag
     91 
     92 


RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre></div></div>

<h3 id="wrap-up">Wrap up</h3>

<ol>
  <li>The <code class="language-plaintext highlighter-rouge">backward()</code> function made differentiation very simple</li>
  <li>For non-scalar <code class="language-plaintext highlighter-rouge">tensor</code>, we need to specify <code class="language-plaintext highlighter-rouge">grad_tensors</code></li>
  <li>If you need to backward() twice on a graph or subgraph, you will need to set <code class="language-plaintext highlighter-rouge">retain_graph</code> to be true.</li>
  <li>Note that grad will accumulate from excuting the graph multiple times.</li>
</ol>
</div><section class="article__sharing d-print-none"></section><div class="d-print-none"><footer class="article__footer"><meta itemprop="dateModified" content="2017-10-24T00:00:00+02:00"><!-- start custom article footer snippet -->

<!-- end custom article footer snippet -->
<div class="article__license"><div class="license">
    <p>This work is licensed under a <a itemprop="license" rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International</a> license.
      <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">
        <img alt="Attribution-NonCommercial 4.0 International" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
      </a>
    </p>
  </div></div></footer>
<div class="article__section-navigator clearfix"><div class="previous"><span>PREVIOUS</span><a href="/stats/2015/07/12/Bayesian-basics1-way-of-reasoning.html">Bayesian basics I - the way of reasoning</a></div><div class="next"><span>NEXT</span><a href="/tech/2017/10/24/Setting-Jekyll-on-Ubuntu-with-LaTex-support.html">Setting Jekyll on Ubuntu 16.04 with Latex support</a></div></div></div>

</div>

<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    $(function() {
      var $this ,$scroll;
      var $articleContent = $('.js-article-content');
      var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
      var scroll = hasSidebar ? '.js-page-main' : 'html, body';
      $scroll = $(scroll);

      $articleContent.find('.highlight').each(function() {
        $this = $(this);
        $this.attr('data-lang', $this.find('code').attr('data-lang'));
      });
      $articleContent.find('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]').each(function() {
        $this = $(this);
        $this.append($('<a class="anchor d-print-none" aria-hidden="true"></a>').html('<i class="fas fa-anchor"></i>'));
      });
      $articleContent.on('click', '.anchor', function() {
        $scroll.scrollToAnchor('#' + $(this).parent().attr('id'), 400);
      });
    });
  });
})();
</script>
</div><section class="page__comments d-print-none"></section></article><!-- start custom main bottom snippet -->

<!-- end custom main bottom snippet -->
</div>
            </div></div></div><div class="page__footer d-print-none">
<footer class="footer py-4 js-page-footer">
  <div class="main"><div itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Linlin Zhao"><meta itemprop="url" content="/"><meta itemprop="description" content="I am a data scientist, an eager learner and a happy father"><div class="footer__author-links"><div class="author-links">
  <ul class="menu menu--nowrap menu--inline"></ul>
</div>
</div>
    </div><div class="site-info mt-2">
      <div>© Linlin's blog 2021,
        Powered by <a title="Jekyll is a simple, blog-aware, static site generator." href="http://jekyllrb.com/">Jekyll</a> & <a
        title="TeXt is a super customizable Jekyll theme." href="https://github.com/kitian616/jekyll-TeXt-theme">TeXt Theme</a>.
      </div>
    </div>
  </div>
</footer>
</div></div>
    </div><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $body = $('body'), $window = $(window);
    var $pageRoot = $('.js-page-root'), $pageMain = $('.js-page-main');
    var activeCount = 0;
    function modal(options) {
      var $root = this, visible, onChange, hideWhenWindowScroll = false;
      var scrollTop;
      function setOptions(options) {
        var _options = options || {};
        visible = _options.initialVisible === undefined ? false : show;
        onChange = _options.onChange;
        hideWhenWindowScroll = _options.hideWhenWindowScroll;
      }
      function init() {
        setState(visible);
      }
      function setState(isShow) {
        if (isShow === visible) {
          return;
        }
        visible = isShow;
        if (visible) {
          activeCount++;
          scrollTop = $(window).scrollTop() || $pageMain.scrollTop();
          $root.addClass('modal--show');
          $pageMain.scrollTop(scrollTop);
          activeCount === 1 && ($pageRoot.addClass('show-modal'), $body.addClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.on('scroll', hide);
          $window.on('keyup', handleKeyup);
        } else {
          activeCount > 0 && activeCount--;
          $root.removeClass('modal--show');
          $window.scrollTop(scrollTop);
          activeCount === 0 && ($pageRoot.removeClass('show-modal'), $body.removeClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.off('scroll', hide);
          $window.off('keyup', handleKeyup);
        }
        onChange && onChange(visible);
      }
      function show() {
        setState(true);
      }
      function hide() {
        setState(false);
      }
      function handleKeyup(e) {
        // Char Code: 27  ESC
        if (e.which ===  27) {
          hide();
        }
      }
      setOptions(options);
      init();
      return {
        show: show,
        hide: hide,
        $el: $root
      };
    }
    $.fn.modal = modal;
  });
})();
</script><div class="modal modal--overflow page__search-modal d-print-none js-page-search-modal"><script>
(function () {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    // search panel
    var search = (window.search || (window.search = {}));
    var useDefaultSearchBox = window.useDefaultSearchBox === undefined ?
      true : window.useDefaultSearchBox ;

    var $searchModal = $('.js-page-search-modal');
    var $searchToggle = $('.js-search-toggle');
    var searchModal = $searchModal.modal({ onChange: handleModalChange, hideWhenWindowScroll: true });
    var modalVisible = false;
    search.searchModal = searchModal;

    var $searchBox = null;
    var $searchInput = null;
    var $searchClear = null;

    function getModalVisible() {
      return modalVisible;
    }
    search.getModalVisible = getModalVisible;

    function handleModalChange(visible) {
      modalVisible = visible;
      if (visible) {
        search.onShow && search.onShow();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].focus();
      } else {
        search.onShow && search.onHide();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].blur();
        setTimeout(function() {
          useDefaultSearchBox && ($searchInput.val(''), $searchBox.removeClass('not-empty'));
          search.clear && search.clear();
          window.pageAsideAffix && window.pageAsideAffix.refresh();
        }, 400);
      }
    }

    $searchToggle.on('click', function() {
      modalVisible ? searchModal.hide() : searchModal.show();
    });
    // Char Code: 83  S, 191 /
    $(window).on('keyup', function(e) {
      if (!modalVisible && !window.isFormElement(e.target || e.srcElement) && (e.which === 83 || e.which === 191)) {
        modalVisible || searchModal.show();
      }
    });

    if (useDefaultSearchBox) {
      $searchBox = $('.js-search-box');
      $searchInput = $searchBox.children('input');
      $searchClear = $searchBox.children('.js-icon-clear');
      search.getSearchInput = function() {
        return $searchInput.get(0);
      };
      search.getVal = function() {
        return $searchInput.val();
      };
      search.setVal = function(val) {
        $searchInput.val(val);
      };

      $searchInput.on('focus', function() {
        $(this).addClass('focus');
      });
      $searchInput.on('blur', function() {
        $(this).removeClass('focus');
      });
      $searchInput.on('input', window.throttle(function() {
        var val = $(this).val();
        if (val === '' || typeof val !== 'string') {
          search.clear && search.clear();
        } else {
          $searchBox.addClass('not-empty');
          search.onInputNotEmpty && search.onInputNotEmpty(val);
        }
      }, 400));
      $searchClear.on('click', function() {
        $searchInput.val(''); $searchBox.removeClass('not-empty');
        search.clear && search.clear();
      });
    }
  });
})();
</script><div class="search search--dark">
  <div class="main">
    <div class="search__header">Search</div>
    <div class="search-bar">
      <div class="search-box js-search-box">
        <div class="search-box__icon-search"><i class="fas fa-search"></i></div>
        <input type="text" />
        <div class="search-box__icon-clear js-icon-clear">
          <a><i class="fas fa-times"></i></a>
        </div>
      </div>
      <button class="button button--theme-dark button--pill search__cancel js-search-toggle">
        Cancel</button>
    </div>
    <div class="search-result js-search-result"></div>
  </div>
</div>
<script>var SOURCES = window.TEXT_VARIABLES.sources;
var PAHTS = window.TEXT_VARIABLES.paths;
window.Lazyload.js([SOURCES.jquery, PAHTS.search_js], function() {
  var search = (window.search || (window.search = {}));
  var searchData = window.TEXT_SEARCH_DATA || {};

  function memorize(f) {
    var cache = {};
    return function () {
      var key = Array.prototype.join.call(arguments, ',');
      if (key in cache) return cache[key];
      else return cache[key] = f.apply(this, arguments);
    };
  }

  /// search
  function searchByQuery(query) {
    var i, j, key, keys, cur, _title, result = {};
    keys = Object.keys(searchData);
    for (i = 0; i < keys.length; i++) {
      key = keys[i];
      for (j = 0; j < searchData[key].length; j++) {
        cur = searchData[key][j], _title = cur.title;
        if ((result[key] === undefined || result[key] && result[key].length < 4 )
          && _title.toLowerCase().indexOf(query.toLowerCase()) >= 0) {
          if (result[key] === undefined) {
            result[key] = [];
          }
          result[key].push(cur);
        }
      }
    }
    return result;
  }

  var renderHeader = memorize(function(header) {
    return $('<p class="search-result__header">' + header + '</p>');
  });

  var renderItem = function(index, title, url) {
    return $('<li class="search-result__item" data-index="' + index + '"><a class="button" href="' + url + '">' + title + '</a></li>');
  };

  function render(data) {
    if (!data) { return null; }
    var $root = $('<ul></ul>'), i, j, key, keys, cur, itemIndex = 0;
    keys = Object.keys(data);
    for (i = 0; i < keys.length; i++) {
      key = keys[i];
      $root.append(renderHeader(key));
      for (j = 0; j < data[key].length; j++) {
        cur = data[key][j];
        $root.append(renderItem(itemIndex++, cur.title, cur.url));
      }
    }
    return $root;
  }

  // search box
  var $result = $('.js-search-result'), $resultItems;
  var lastActiveIndex, activeIndex;

  function clear() {
    $result.html(null);
    $resultItems = $('.search-result__item'); activeIndex = 0;
  }
  function onInputNotEmpty(val) {
    $result.html(render(searchByQuery(val)));
    $resultItems = $('.search-result__item'); activeIndex = 0;
    $resultItems.eq(0).addClass('active');
  }

  search.clear = clear;
  search.onInputNotEmpty = onInputNotEmpty;

  function updateResultItems() {
    lastActiveIndex >= 0 && $resultItems.eq(lastActiveIndex).removeClass('active');
    activeIndex >= 0 && $resultItems.eq(activeIndex).addClass('active');
  }

  function moveActiveIndex(direction) {
    var itemsCount = $resultItems ? $resultItems.length : 0;
    if (itemsCount > 1) {
      lastActiveIndex = activeIndex;
      if (direction === 'up') {
        activeIndex = (activeIndex - 1 + itemsCount) % itemsCount;
      } else if (direction === 'down') {
        activeIndex = (activeIndex + 1 + itemsCount) % itemsCount;
      }
      updateResultItems();
    }
  }

  // Char Code: 13  Enter, 37  ⬅, 38  ⬆, 39  ➡, 40  ⬇
  $(window).on('keyup', function(e) {
    var modalVisible = search.getModalVisible && search.getModalVisible();
    if (modalVisible) {
      if (e.which === 38) {
        modalVisible && moveActiveIndex('up');
      } else if (e.which === 40) {
        modalVisible && moveActiveIndex('down');
      } else if (e.which === 13) {
        modalVisible && $resultItems && activeIndex >= 0 && $resultItems.eq(activeIndex).children('a')[0].click();
      }
    }
  });

  $result.on('mouseover', '.search-result__item > a', function() {
    var itemIndex = $(this).parent().data('index');
    itemIndex >= 0 && (lastActiveIndex = activeIndex, activeIndex = itemIndex, updateResultItems());
  });
});
</script>
</div></div>


<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function scrollToAnchor(anchor, duration, callback) {
      var $root = this;
      $root.animate({ scrollTop: $(anchor).position().top }, duration, function() {
        window.history.replaceState(null, '', window.location.href.split('#')[0] + anchor);
        callback && callback();
      });
    }
    $.fn.scrollToAnchor = scrollToAnchor;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function affix(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroll,
        offsetBottom = 0, scrollTarget = window, scroll = window.document, disabled = false, isOverallScroller = true,
        rootTop, rootLeft, rootHeight, scrollBottom, rootBottomTop,
        hasInit = false, curState;

      function setOptions(options) {
        var _options = options || {};
        _options.offsetBottom && (offsetBottom = _options.offsetBottom);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroll && (scroll = _options.scroll);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $scrollTarget = $(scrollTarget);
        isOverallScroller = window.isOverallScroller($scrollTarget[0]);
        $scroll = $(scroll);
      }
      function preCalc() {
        top();
        rootHeight = $root.outerHeight();
        rootTop = $root.offset().top + (isOverallScroller ? 0 :  $scrollTarget.scrollTop());
        rootLeft = $root.offset().left;
      }
      function calc(needPreCalc) {
        needPreCalc && preCalc();
        scrollBottom = $scroll.outerHeight() - offsetBottom - rootHeight;
        rootBottomTop = scrollBottom - rootTop;
      }
      function top() {
        if (curState !== 'top') {
          $root.removeClass('fixed').css({
            left: 0,
            top: 0
          });
          curState = 'top';
        }
      }
      function fixed() {
        if (curState !== 'fixed') {
          $root.addClass('fixed').css({
            left: rootLeft + 'px',
            top: 0
          });
          curState = 'fixed';
        }
      }
      function bottom() {
        if (curState !== 'bottom') {
          $root.removeClass('fixed').css({
            left: 0,
            top: rootBottomTop + 'px'
          });
          curState = 'bottom';
        }
      }
      function setState() {
        var scrollTop = $scrollTarget.scrollTop();
        if (scrollTop >= rootTop && scrollTop <= scrollBottom) {
          fixed();
        } else if (scrollTop < rootTop) {
          top();
        } else {
          bottom();
        }
      }
      function init() {
        if(!hasInit) {
          var interval, timeout;
          calc(true); setState();
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState();
          });
          $window.on('resize', function() {
            disabled || (calc(true), setState());
          });
          hasInit = true;
        }
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions,
        refresh: function() {
          calc(true, { animation: false }); setState();
        }
      };
    }
    $.fn.affix = affix;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function toc(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroller, $tocUl = $('<ul class="toc toc--ellipsis"></ul>'), $tocLi, $headings, $activeLast, $activeCur,
        selectors = 'h1,h2,h3', container = 'body', scrollTarget = window, scroller = 'html, body', disabled = false,
        headingsPos, scrolling = false, hasRendered = false, hasInit = false;

      function setOptions(options) {
        var _options = options || {};
        _options.selectors && (selectors = _options.selectors);
        _options.container && (container = _options.container);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroller && (scroller = _options.scroller);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $headings = $(container).find(selectors).filter('[id]');
        $scrollTarget = $(scrollTarget);
        $scroller = $(scroller);
      }
      function calc() {
        headingsPos = [];
        $headings.each(function() {
          headingsPos.push(Math.floor($(this).position().top));
        });
      }
      function setState(element, disabled) {
        var scrollTop = $scrollTarget.scrollTop(), i;
        if (disabled || !headingsPos || headingsPos.length < 1) { return; }
        if (element) {
          $activeCur = element;
        } else {
          for (i = 0; i < headingsPos.length; i++) {
            if (scrollTop >= headingsPos[i]) {
              $activeCur = $tocLi.eq(i);
            } else {
              $activeCur || ($activeCur = $tocLi.eq(i));
              break;
            }
          }
        }
        $activeLast && $activeLast.removeClass('active');
        ($activeLast = $activeCur).addClass('active');
      }
      function render() {
        if(!hasRendered) {
          $root.append($tocUl);
          $headings.each(function() {
            var $this = $(this);
            $tocUl.append($('<li></li>').addClass('toc-' + $this.prop('tagName').toLowerCase())
              .append($('<a></a>').text($this.text()).attr('href', '#' + $this.prop('id'))));
          });
          $tocLi = $tocUl.children('li');
          $tocUl.on('click', 'a', function(e) {
            e.preventDefault();
            var $this = $(this);
            scrolling = true;
            setState($this.parent());
            $scroller.scrollToAnchor($this.attr('href'), 400, function() {
              scrolling = false;
            });
          });
        }
        hasRendered = true;
      }
      function init() {
        var interval, timeout;
        if(!hasInit) {
          render(); calc(); setState(null, scrolling);
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState(null, scrolling);
          });
          $window.on('resize', window.throttle(function() {
            if (!disabled) {
              render(); calc(); setState(null, scrolling);
            }
          }, 100));
        }
        hasInit = true;
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions
      };
    }
    $.fn.toc = toc;
  });
})();
/*(function () {

})();*/
</script><script>
  /* toc must before affix, since affix need to konw toc' height. */(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  var TOC_SELECTOR = window.TEXT_VARIABLES.site.toc.selectors;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window);
    var $articleContent = $('.js-article-content');
    var $tocRoot = $('.js-toc-root'), $col2 = $('.js-col-aside');
    var toc;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
    var hasToc = $articleContent.find(TOC_SELECTOR).length > 0;

    function disabled() {
      return $col2.css('display') === 'none' || !hasToc;
    }

    tocDisabled = disabled();

    toc = $tocRoot.toc({
      selectors: TOC_SELECTOR,
      container: $articleContent,
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      tocDisabled = disabled();
      toc && toc.setOptions({
        disabled: tocDisabled
      });
    }, 100));

  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window), $pageFooter = $('.js-page-footer');
    var $pageAside = $('.js-page-aside');
    var affix;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');

    affix = $pageAside.affix({
      offsetBottom: $pageFooter.outerHeight(),
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      scroll: hasSidebar ? $('.js-page-main').children() : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      affix && affix.setOptions({
        disabled: tocDisabled
      });
    }, 100));

    window.pageAsideAffix = affix;
  });
})();
</script><!--script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> 
</script --><script type="text/x-mathjax-config">
	var _config = { tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']]
	}};MathJax.Hub.Config(_config);
</script>
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>

    </div>
    <script>(function () {
  var $root = document.getElementsByClassName('root')[0];
  if (window.hasEvent('touchstart')) {
    $root.dataset.isTouch = true;
    document.addEventListener('touchstart', function(){}, false);
  }
})();
</script>
  </body>
</html>

