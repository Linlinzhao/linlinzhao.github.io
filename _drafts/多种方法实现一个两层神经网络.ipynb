{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好地熟悉PyTorch和对比其与其他框架的区别，将官网上的例程自己都写一遍并做更详细的注释。例程中，只快速实现两层神经网络的核心部分，因此训练数据是随机生成的，而且只实现了对参数的更新调整，未涉及对代价函数的优化过程。完成全套例程，会对神经网络的前向通道及反向传播有更好的理解。具体实现的方法有：\n",
    "\n",
    "1. 利用Numpy实现 (CPU)\n",
    "2. 利用PyTorch的tensor实现 (CPU和GPU)\n",
    "3. 利用PyTorch的autograd模块实现\n",
    "4. 利用Tensorflow实现，对比静态图与动态图的区别\n",
    "\n",
    "#### 1. Numpy实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#先定义网络结构: batch_size, Input Dimension, Hidden Dimension, Output Dimension \n",
    "N, D_in, D_hidden, D_out = 10, 20, 30, 5  \n",
    "\n",
    "#随机生成输入和输出数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "#对输入层和输出层的参数进行初始化\n",
    "w1 = np.random.randn(D_in, D_hidden)\n",
    "w2 = np.random.randn(D_hidden, D_out)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#循环更新参数，每个循环前向和反向各计算一次\n",
    "for i in xrange(50):\n",
    "    \n",
    "    # 计算前向通道\n",
    "    h_linear = x.dot(w1)     #10x20 and 20x30 produce 10x30, which is the shape of h_linear\n",
    "    h_relu = np.maximum(h_linear, 0) #note one have to use np.maximum but not np.max, 10x30\n",
    "    y_pred = h_relu.dot(w2)  #10x30 and 30x5 produce 10x5\n",
    "    \n",
    "    #定义代价函数\n",
    "    loss = 0.5 * np.sum(np.square(y_pred - y))  #sum squared error as loss\n",
    "    \n",
    "    # 反向求导\n",
    "    grad_y_pred = y_pred - y   #10x5\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)    #30x10 and 10x5 produce the dimension of w2: 30x5\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)      #30x5 and 10x5 produce the dimension of h_relu: 10x30\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h_linear < 0] = 0     #替代针对隐含层导数中的负数为零\n",
    "    grad_w1 = x.T.dot(grad_h)    #20x10 and 10x30 produce 20x30 \n",
    "    \n",
    "    #梯度下降法更新参数\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PyTorch的tensor实现\n",
    "\n",
    "只需将numpy的程序稍作调整就能实现tensor的实现，从而是程序能够部署到GPU上运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "\n",
    "#先定义网络结构: batch_size, Input Dimension, Hidden Dimension, Output Dimension \n",
    "N, D_in, D_hidden, D_out = 10, 20, 30, 5  \n",
    "\n",
    "#随机生成输入和输出数据\n",
    "x = T.randn(N, D_in)\n",
    "y = T.randn(N, D_out)\n",
    "\n",
    "#对输入层和输出层的参数进行初始化\n",
    "w1 = T.randn(D_in, D_hidden)\n",
    "w2 = T.randn(D_hidden, D_out)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#循环更新参数，每个循环前向和反向各计算一次\n",
    "for i in xrange(50):\n",
    "    \n",
    "    # 计算前向通道\n",
    "    #mm should also work as x is a matrix. The matrix multiplication will be summarized in another post\n",
    "    h_linear = x.matmul(w1)     #10x20 and 20x30 produce 10x30, which is the shape of h_linear\n",
    "    h_relu = h_linear.clamp(min=0) #note one have to use np.maximum but not np.max, 10x30\n",
    "    y_pred = h_relu.matmul(w2)  #10x30 and 30x5 produce 10x5\n",
    "    \n",
    "    #定义代价函数\n",
    "    loss = 0.5 * (y_pred - y).pow(2).sum() #sum squared error as loss\n",
    "    \n",
    "    # 反向求导\n",
    "    grad_y_pred = y_pred - y   #10x5\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)    #30x10 and 10x5 produce the dimension of w2: 30x5\n",
    "    grad_h_relu = grad_y_pred.dot(w2.t())      #30x5 and 10x5 produce the dimension of h_relu: 10x30\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h_linear < 0] = 0     #替代针对隐含层导数中的负数为零\n",
    "    grad_w1 = x.t().mm(grad_h)    #20x10 and 10x30 produce 20x30 \n",
    "    \n",
    "    #梯度下降法更新参数\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 利用PyTorch的Tensor和autograd实现\n",
    "\n",
    "两层网络的反向求导比较容易，但如果层数加多，在手动求导就会变得很复杂。因此深度学习平台都提供了自动求导功能，PyTorch的Autograd中的自动求导功能可以使反向求导简捷且灵活。要注意的是计算图的构建需要用autograd中的Variable将需要并入计算图中的变量进行封装，并设置相关属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#先定义网络结构: batch_size, Input Dimension, Hidden Dimension, Output Dimension \n",
    "N, D_in, D_hidden, D_out = 10, 20, 30, 5  \n",
    "\n",
    "#随机生成输入和输出数据, 并用Variable对输入输出进行封装，同时在计算图形中不要求求导\n",
    "x = Variable(T.randn(N, D_in), requires_grad=False)\n",
    "y = Variable(T.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "#对输入层和输出层的参数进行初始化，并用Variable封装，同时要求求导\n",
    "w1 = Variable(T.randn(D_in, D_hidden), requires_grad=True)\n",
    "w2 = Variable(T.randn(D_hidden, D_out), requires_grad=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#循环更新参数，每个循环前向和反向各计算一次\n",
    "for i in xrange(50):\n",
    "    \n",
    "    # 计算前向通道\n",
    "    #mm should also work as x is a matrix. The matrix multiplication will be summarized in another post\n",
    "    h_linear = x.matmul(w1)     #10x20 and 20x30 produce 10x30, which is the shape of h_linear\n",
    "    h_relu = h_linear.clamp(min=0) #note one have to use np.maximum but not np.max, 10x30\n",
    "    y_pred = h_relu.matmul(w2)  #10x30 and 30x5 produce 10x5\n",
    "    \n",
    "    #定义代价函数\n",
    "    loss = 0.5 * (y_pred - y).pow(2).sum() #sum squared error as loss\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    #梯度下降法更新参数\n",
    "    w1.data -= learning_rate * w1.grad.data  #note that we are updating the 'data' of Variable w1\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "    \n",
    "    #PyTorch中，将grad中的值在循环中进行累积，当不须此操作时，应清零\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 利用tensorflow实现\n",
    "\n",
    "tensorflow和PyTorch的核心区别在于，前者使用静态图，即在实际导入数据前就已经把完整的计算图形定义了，在训练过程对此图形进行参数更新；而后者作为动态图，在每个循环中都可以改变计算图形，比如添加或减少图形中的计算点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
