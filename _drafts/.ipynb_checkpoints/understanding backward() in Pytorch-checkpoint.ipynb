{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having heard about the announcement about Theano from Bengio lab , as a Theano user, I am happy and sad to see the fading of the old hero, caused by many raising stars. Sad to see it is too old to compete with its industrial competitors, and happy to have so many excellent deep learning frameworks to choose from. Recently I started translating some of my old codes to Pytorch and have been really impressed by its dynamic nature and clearness. But at the very beginning, I was very confused by the ``backward()`` function when reading the tutorials and documentations. This motivated me to write this post in order for other Pytorch beginners to ease the understanding a bit. And I'll assume that you already know the [``autograd``](http://pytorch.org/docs/master/autograd.html) module and what a [``Variable``](http://pytorch.org/docs/0.1.12/_modules/torch/autograd/variable.html) is, but are a little confused by definition of ``backward()``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's recall the gradient computing under mathematical notions. For an independent variable $x$ (scalar or vector), the first whatever operation on $x$ is $y = f(x)$. Then the gradient of $y$ w.r.t $x_i$s is\n",
    "$$\\begin{align}\\nabla y&=\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial x_1}\\\\\n",
    "\\frac{\\partial y}{\\partial x_2}\\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "\\end{align}.\n",
    "$$\n",
    "Then for a specific point of $x=[X_1, X_2, \\dots]$, we'll get the gradient of $y$ on that point as a vector. With these notions in my mind, the confusing part of the function ``torch.autograd.backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None)`` is the parameter ``grad_variables``. And as the function returns the sum of gradients of ``Variables`` w.r.t leaf variables, then the attribute ``.grad`` of leaf variable made me deep in the cloud. Okay, an example is better than thousands of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as T\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      "-0.8175\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "y Variable containing:\n",
      "-3.2701\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "z Variable containing:\n",
      "-34.9702\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Define a scalar variable, set requires_grad to be true to add it to backward path for computing gradients'''\n",
    "x = Variable(T.randn(1, 1), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "print('z', z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There simple operations defined a forward path $z=(2x)^3$, $z$ will be the final output ``Variable`` we would like to compute gradient $dz=24x^2dx$, which will be passed to the parameter ``Variables`` in ``backward()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yes, it is just as simple as this:\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 128.3257\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients of both $y$ and $z$ are None, since the function returns the gradient for the leaves, which is $x$ in this case. At the very beginning, I was assuming something like this:\n",
    "\n",
    "``x gradient None\n",
    "y gradient None\n",
    "z gradient Variable containing:\n",
    " 128.3257\n",
    "[torch.FloatTensor of size 1x1]``,\n",
    "since the gradient is for the final output $z$.\n",
    "\n",
    "With a blink of thinking, we could clarify that would be practically chaos if $x$ is a multi-dimensional vector. ``x.grad`` should be interpreted as the gradient of $z$ at $x$. Till now, we know the simple usage of ``backward()``. But wait, what the heck is ``grad_variables``? Let's do the above again with some values for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      "-1.2095\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "y Variable containing:\n",
      "-2.4190\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "z Variable containing:\n",
      "-14.1552\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Keeping the default value gives\n",
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 35.1098\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(1, 1), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "print('z', z)\n",
    "z.backward(T.FloatTensor([1]), retain_graph=True)\n",
    "print('Keeping the default value gives')\n",
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 3.5110\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "z.backward(T.FloatTensor([0.1]), retain_graph=True)\n",
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that if the value is set to be 0.1, then the gradient became one tenth of the original gradient. Do one more time to set it to be 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 4.4785\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "z.backward(T.FloatTensor([0.2]), retain_graph=True)\n",
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add one dimension to $x$. We can clearly see the gradients of $z$ are computed w.r.t to each dimension of $x$. Note that in this case $z$ is also two-dimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 1.3448 -0.2924\n",
      "-0.4746  0.8842\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "z Variable containing:\n",
      " 19.4567  -0.2000\n",
      " -0.8550   5.5296\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 43.4041   0.0000\n",
      "  5.4049   0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 43.4041   2.0518\n",
      "  5.4049  18.7622\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(2, 2), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "print('z', z)\n",
    "z.backward(T.FloatTensor([1, 0]), retain_graph=True)\n",
    "print('x gradient', x.grad)\n",
    "z.backward(T.FloatTensor([0, 1]), retain_graph=True)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then what if we render the output one-dimensional (scalar) while $x$ is two-dimensional. This is a real simplified scenario of neural networks. \n",
    "$$f(x)=\\frac{1}{n}\\sum_i^n(2x_i)^3$$\n",
    "$$f'(x)=\\frac{1}{n}\\sum_i^n24x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      "-0.4230\n",
      "-1.2352\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "out Variable containing:\n",
      "-7.8402\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "x gradient Variable containing:\n",
      "  2.1467\n",
      " 18.3074\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(2, 1), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "out = z.mean()\n",
    "print('out', out)\n",
    "out.backward(T.FloatTensor([1]), retain_graph=True)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
