{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple way\n",
    "\n",
    "The simplest way of dealing with text is building a frequency matrix where the rows are words, \n",
    "the columns are the articles, and entries are the frequencies of the words in the corresponding \n",
    "articles. If SVD or non-negative matrix factorization is applied on such matrix, $$U$$ matrix \n",
    "the reduced-dimension representation of the columns, i.e., the articles, $V$ matrix is for words. \n",
    "Further, if we would like to visualize the $U$ or $V$ matrix, techniques such as tSNE can be applied\n",
    "to project the matrices to lower dimension by some kind of recombination and transformation. \n",
    "The columns of $U$ are the bases of articles, the rows of $V$ are the bases of words.\n",
    "\n",
    "### Word2vec and GloVe\n",
    "\n",
    "They are based on the idea of Co-occurence matrix in order to capture the context information. \n",
    "A window size is required to construct such a matrix, which is a hyper-parameter need to be\n",
    "tuned during learning. The window length could be somehow interpreted as a correlation length. \n",
    "With a fixed window size, the corpus is transformed a large co-occurrence matrix.\n",
    "\n",
    "### Continuous bag of words and skip-gram\n",
    "\n",
    "CBOW predicts the current word according to its context while skip-gram predicts the current word's context.\n",
    "\n",
    "The core idea of CBOW is to use neural network to predict the probability of the current word given its context,\n",
    "i.e. $$p(w|c)$$, noting that the output layer is softmax which needs a normalization term which summarize all words. The natural choice of objective function for training then becomes the maximum likelihood.\n",
    "\n",
    "###Problem in practice\n",
    "Usually the size corpus is very large, which makes the normalization term intractable to calculate in practice. To resolve this problem, researcher came up the idea of negative sampling.\n",
    "\n",
    "The idea here is to treat the problem as a classification problem.\n",
    "\n",
    "* Given a fixed length window, we can construct a set $D$ of pairs (w, c) in the text, i.e. the word and its context truly appear in the text.\n",
    "* Similarly, we can also construct another set $D'$ of pairs (w, c) which are not in the text.\n",
    "* Then the classification problem becomes to classify if the given pair (w, c) shows up the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
