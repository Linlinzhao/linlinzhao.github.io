{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as T\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络算法实现的核心之一是对代价函数的反向求导，Theano和Tensorflow中都定义了求导的符号函数，同样地，作为深度学习平台，自动求导（``autograd``）功能在pytorch中也扮演着核心功能，不同的是，pytorch的动态图功能使其更灵活（define by run）， 比如甚至在每次迭代中都可以通过改变pytorch中Variable的属性，从而使其加入亦或退出反向求导图，这个功能在某些应用中会特别使用，比如在训练的后半段，我们需要更新的只有后层的参数，那么只需要在前层参数的Variable设置成不要求求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 变量\n",
    "\n",
    "``autograd.Variable``是pytorch设计理念的核心之一，它将tensor封装成Variable，并支持绝大多数tensor上的操作，同时赋予其两个极其重要的属性：``requires_grad``和``volatile``，Variable还具有三个属性： ``.data``用于存储Variable的数值，``.grad``也为变量，用于存储Variable的导数值,``.grad_fn `` (creator) 是生成该Variable的函数，当Variable为用户自定义时，其值为\"None\". 细节参见源码[Variable](http://pytorch.org/docs/0.1.12/_modules/torch/autograd/variable.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.ones(2,2), requires_grad=True)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 20.0855  20.0855\n",
      " 20.0855  20.0855\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = T.exp(x + 2)\n",
    "yy = T.exp(-x-2)\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 10.0677  10.0677\n",
      " 10.0677  10.0677\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 10.0677\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = (y + yy)/2\n",
    "out = z.mean()\n",
    "print z, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"100pt\" height=\"29pt\"\n",
       " viewBox=\"0.00 0.00 100.00 29.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 25)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-25 96,-25 96,4 -4,4\"/>\n",
       "<!-- 140396231155792 -->\n",
       "<g id=\"node1\" class=\"node\"><title>140396231155792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"92,-21 0,-21 0,-0 92,-0 92,-21\"/>\n",
       "<text text-anchor=\"middle\" x=\"46\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MeanBackward</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fb08b79c350>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out.backward(T.FloatTensor(1), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.2072e+21 -1.2072e+21\n",
       "-1.2072e+21 -1.2072e+21\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.7466\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.1988\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      " 2.1578\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "xx = Variable(torch.randn(1,1), requires_grad = True)\n",
    "print(xx)\n",
    "yy = 3*xx\n",
    "zz = yy**2\n",
    "\n",
    "#yy.register_hook(print)\n",
    "zz.backward(T.FloatTensor([0.1]))\n",
    "print(xx.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A simple numpy implementation of one hidden layer neural network. \n",
    "\n",
    "In this implementation, for each update of $w_i$, both the forward and backward passes need to computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred = w2*(relu(w1*x))\n",
    "# loss = 0.5*sum (y_pred - y)^2\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, D_hidden, D_out = 50, 40, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, D_hidden)\n",
    "w2 = np.random.randn(D_hidden, D_out)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "for t in range(100):\n",
    "    ### 前向通道\n",
    "    h = x.dot(w1) #50x40 and 40x100 produce 50x100\n",
    "    h_relu = np.maximum(h, 0)  #this has to be np.maximum as it takes two input arrays and do element-wise max, 50x100\n",
    "    y_pred = h_relu.dot(w2) #50x100 and 100x10 produce 50x10\n",
    "    #print y_pred.shape\n",
    "    \n",
    "    ### 误差函数\n",
    "    loss = 0.5 * np.sum(np.square(y_pred - y))\n",
    "    \n",
    "    \n",
    "    ### 反向通道\n",
    "    grad_y_pred = y_pred - y #50x10\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) #50x100 and 50x10 should produce 100x10, so transpose h_relu\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) #50x10 and 100x10 should produce 50x100, so transpose w2\n",
    "    grad_h = grad_h_relu.copy() #make a copy of \n",
    "    grad_h[grad_h < 0] = 0      #\n",
    "    grad_w1 = x.T.dot(grad_h)     #50x100 and 50x40 should produce 40x100\n",
    "    \n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with very slight modifications, we could end up with the implementation of the same algorithm in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, D_hidden, D_out = 50, 40, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, D_hidden)\n",
    "w2 = torch.randn(D_hidden, D_out)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "for t in range(100):\n",
    "    h = x.mm(w1) #50x40 and 40x100 produce 50x100\n",
    "    #h = x.matmul(w1) #50x40 and 40x100 produce 50x100, matmul for checking\n",
    "    h_relu = h.clamp(min=0)  #this has to be np.maximum as it takes two input arrays and do element-wise max, 50x100\n",
    "    y_pred = h_relu.mm(w2) #50x100 and 100x10 produce 50x10\n",
    "    #print y_pred.shape\n",
    "    \n",
    "    loss = 0.5 * (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    grad_y_pred = y_pred - y #50x10\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred) #50x100 and 50x10 should produce 100x10, so transpose h_relu\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t()) #50x10 and 100x10 should produce 50x100, so transpose w2\n",
    "    grad_h = grad_h_relu.clone() #make a copy\n",
    "    grad_h[grad_h < 0] = 0      #\n",
    "    grad_w1 = x.t().mm(grad_h)     #50x100 and 50x40 should produce 40x100\n",
    "    \n",
    "    w1 = w1 - learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now with the autograd functionality in PyTorch, we could see the ease of doing backpropagation, calculating gradients for two layers networks is not a big deal but it becomes much more complicated when the number of layers grows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic matrix multiplication in Pytorch\n",
    "\n",
    "PyTorch provided ``torch.dot()``, ``torch.mm()``, ``torch.matmul()``, ``*``, for basic matrix multiplication. It is worthy of noting the differences among them, ``torch.dot(a, b)`` gives the inner product of 1-D vectors $a$ and $b$, ``torch.mm(a, b)`` gives the matrix multiplication of 2-D matrices, and ``torch.matmul()`` operates on two tensors. It seems that ``torch.matmul()`` can replace both ``torch.dot()`` and ``torch.mm()``, but not vice versa. Finally ``*`` simply calculates the elementwise products, i.e. the Hadamard product. \n",
    "\n",
    "#### Advanced matrix multiplication\n",
    "\n",
    "``torch.bmm(A, B)``: batch matrix multiplication for 3D tensors, $A_{b\\times n\\times p}$ and $B_{b\\times p\\times m}$ will produce 3D tensor of shape $b\\times n\\times m$\n",
    "\n",
    "``torch.baddbmm(A, B)``:\n",
    "\n",
    "``torch.addbmm(A, B)``:\n",
    "\n",
    "``torch.addmm(A, B)``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
